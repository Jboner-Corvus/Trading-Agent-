"""
ğŸŒ™ Deamon Dev's Groq Model Implementation
Built with love by Deamon Dev ğŸš€
"""

import time

from groq import Groq
from termcolor import cprint

from .base_model import BaseModel, ModelResponse


def safe_cprint(text, color):
    """Safe print that handles Unicode encoding issues"""
    try:
        cprint(text, color)
    except UnicodeEncodeError:
        # Remove emojis and special characters for Windows compatibility
        clean_text = (
            text.replace("âœ¨", "")
            .replace("âŒ", "")
            .replace("ğŸŒ™", "")
            .replace("ğŸš€", "")
            .replace("âš¡", "")
            .replace("ğŸ’", "")
            .replace("ğŸ“ˆ", "")
            .replace("ğŸ“‰", "")
            .replace("ğŸŒŸ", "")
            .replace("ğŸ¤–", "")
            .replace("ğŸ”¥", "")
            .replace("ğŸ’°", "")
            .replace("â­", "")
            .replace("ğŸ”‘", "")
        )
        # Call cprint directly, not safe_cprint to avoid recursion
        cprint(clean_text, color)


class GroqModel(BaseModel):
    """Implementation for Groq's models"""

    AVAILABLE_MODELS = {
        # Production Models
        "mixtral-8x7b-32768": {
            "description": "Mixtral 8x7B - Production - 32k context",
            "input_price": "$0.27/1M tokens",
            "output_price": "$0.27/1M tokens",
        },
        "gemma2-9b-it": {
            "description": "Google Gemma 2 9B - Production - 8k context",
            "input_price": "$0.10/1M tokens",
            "output_price": "$0.10/1M tokens",
        },
        "llama-3.3-70b-versatile": {
            "description": "Llama 3.3 70B Versatile - Production - 128k context",
            "input_price": "$0.70/1M tokens",
            "output_price": "$0.90/1M tokens",
        },
        "llama-3.1-8b-instant": {
            "description": "Llama 3.1 8B Instant - Production - 128k context",
            "input_price": "$0.10/1M tokens",
            "output_price": "$0.10/1M tokens",
        },
        "llama-guard-3-8b": {
            "description": "Llama Guard 3 8B - Production - 8k context",
            "input_price": "$0.20/1M tokens",
            "output_price": "$0.20/1M tokens",
        },
        "llama3-70b-8192": {
            "description": "Llama 3 70B - Production - 8k context",
            "input_price": "$0.70/1M tokens",
            "output_price": "$0.90/1M tokens",
        },
        "llama3-8b-8192": {
            "description": "Llama 3 8B - Production - 8k context",
            "input_price": "$0.10/1M tokens",
            "output_price": "$0.10/1M tokens",
        },
        # Preview Models
        "deepseek-r1-distill-llama-70b": {
            "description": "DeepSeek R1 Distill Llama 70B - Preview - 128k context",
            "input_price": "$0.70/1M tokens",
            "output_price": "$0.90/1M tokens",
        },
        "llama-3.3-70b-specdec": {
            "description": "Llama 3.3 70B SpecDec - Preview - 8k context",
            "input_price": "$0.70/1M tokens",
            "output_price": "$0.90/1M tokens",
        },
        "llama-3.2-1b-preview": {
            "description": "Llama 3.2 1B - Preview - 128k context",
            "input_price": "$0.05/1M tokens",
            "output_price": "$0.05/1M tokens",
        },
        "llama-3.2-3b-preview": {
            "description": "Llama 3.2 3B - Preview - 128k context",
            "input_price": "$0.07/1M tokens",
            "output_price": "$0.07/1M tokens",
        },
        "qwen/qwen3-32b": {
            "description": "Qwen 3 32B - Production - 32k context",
            "input_price": "$0.50/1M tokens",
            "output_price": "$0.50/1M tokens",
        },
    }

    def __init__(self, api_key: str, model_name: str = "qwen/qwen3-32b", **kwargs):
        try:
            safe_cprint(f"\nğŸŒ™ Deamon Dev's Groq Model Initialization", "cyan")

            # Validate API key
            if not api_key or len(api_key.strip()) == 0:
                raise ValueError("API key is empty or None")

            safe_cprint(f"ğŸ”‘ API Key validation:", "cyan")
            safe_cprint(f"  â”œâ”€ Length: {len(api_key)} chars", "cyan")
            safe_cprint(
                f"  â”œâ”€ Contains whitespace: {'yes' if any(c.isspace() for c in api_key) else 'no'}",
                "cyan",
            )
            safe_cprint(
                f"  â””â”€ Starts with 'gsk_': {'yes' if api_key.startswith('gsk_') else 'no'}",
                "cyan",
            )

            # Validate model name
            safe_cprint(f"\nğŸ“ Model validation:", "cyan")
            safe_cprint(f"  â”œâ”€ Requested: {model_name}", "cyan")
            if model_name not in self.AVAILABLE_MODELS:
                safe_cprint(f"  â””â”€ âŒ Invalid model name", "red")
                safe_cprint("\nAvailable models:", "yellow")
                for available_model, info in self.AVAILABLE_MODELS.items():
                    safe_cprint(f"  â”œâ”€ {available_model}", "yellow")
                    safe_cprint(f"  â”‚  â””â”€ {info['description']}", "yellow")
                raise ValueError(f"Invalid model name: {model_name}")
            safe_cprint(f"  â””â”€ âœ… Model name valid", "green")

            self.model_name = model_name

            # Call parent class initialization
            safe_cprint(f"\nğŸ“¡ Parent class initialization...", "cyan")
            super().__init__(api_key, **kwargs)
            safe_cprint(f"âœ… Parent class initialized", "green")

        except Exception as e:
            safe_cprint(f"\nâŒ Error in Groq model initialization", "red")
            safe_cprint(f"  â”œâ”€ Error type: {type(e).__name__}", "red")
            safe_cprint(f"  â”œâ”€ Error message: {str(e)}", "red")
            if "api_key" in str(e).lower():
                safe_cprint(f"  â”œâ”€ ğŸ”‘ This appears to be an API key issue", "red")
                safe_cprint(f"  â””â”€ Please check your GROQ_API_KEY in .env", "red")
            elif "model" in str(e).lower():
                safe_cprint(f"  â”œâ”€ ğŸ¤– This appears to be a model name issue", "red")
                safe_cprint(
                    f"  â””â”€ Available models: {list(self.AVAILABLE_MODELS.keys())}",
                    "red",
                )
            raise

    def initialize_client(self, **kwargs) -> None:
        """Initialize the Groq client"""
        try:
            safe_cprint(f"\nğŸ”Œ Initializing Groq client...", "cyan")
            safe_cprint(f"  â”œâ”€ API Key length: {len(self.api_key)} chars", "cyan")
            safe_cprint(f"  â”œâ”€ Model name: {self.model_name}", "cyan")

            safe_cprint(f"\n  â”œâ”€ Creating Groq client...", "cyan")
            self.client = Groq(api_key=self.api_key)
            safe_cprint(f"  â”œâ”€ âœ… Groq client created", "green")

            # Get list of available models first
            safe_cprint(f"  â”œâ”€ Fetching available models from Groq API...", "cyan")
            available_models = self.client.models.list()
            api_models = [model.id for model in available_models.data]
            safe_cprint(f"  â”œâ”€ Models available from API: {api_models}", "cyan")

            if self.model_name not in api_models:
                safe_cprint(f"  â”œâ”€ âš ï¸ Requested model not found in API", "yellow")
                safe_cprint(f"  â”œâ”€ Falling back to mixtral-8x7b-32768", "yellow")
                self.model_name = "mixtral-8x7b-32768"

            # Test the connection with a simple completion
            safe_cprint(
                f"  â”œâ”€ Testing connection with model: {self.model_name}", "cyan"
            )
            test_response = self.client.chat.completions.create(
                model=self.model_name,
                messages=[{"role": "user", "content": "Hello"}],
                max_tokens=10,
            )
            safe_cprint(f"  â”œâ”€ âœ… Test response received", "green")
            safe_cprint(
                f"  â”œâ”€ Response content: {test_response.choices[0].message.content}",
                "cyan",
            )

            model_info = self.AVAILABLE_MODELS.get(self.model_name, {})
            safe_cprint(f"  â”œâ”€ âœ¨ Groq model initialized: {self.model_name}", "green")
            safe_cprint(f"  â”œâ”€ Model info: {model_info.get('description', '')}", "cyan")
            safe_cprint(
                f"  â””â”€ Pricing: Input {model_info.get('input_price', '')} | Output {model_info.get('output_price', '')}",
                "yellow",
            )

        except Exception as e:
            safe_cprint(f"\nâŒ Failed to initialize Groq client", "red")
            safe_cprint(f"  â”œâ”€ Error type: {type(e).__name__}", "red")
            safe_cprint(f"  â”œâ”€ Error message: {str(e)}", "red")

            # Check for specific error types
            if "api_key" in str(e).lower():
                safe_cprint(f"  â”œâ”€ ğŸ”‘ This appears to be an API key issue", "red")
                safe_cprint(f"  â”œâ”€ Make sure your GROQ_API_KEY is correct", "red")
                safe_cprint(f"  â””â”€ Key length: {len(self.api_key)} chars", "red")
            elif "model" in str(e).lower():
                safe_cprint(f"  â”œâ”€ ğŸ¤– This appears to be a model name issue", "red")
                safe_cprint(f"  â”œâ”€ Requested model: {self.model_name}", "red")
                safe_cprint(
                    f"  â””â”€ Available models: {list(self.AVAILABLE_MODELS.keys())}",
                    "red",
                )

            if hasattr(e, "response"):
                safe_cprint(f"  â”œâ”€ Response status: {e.response.status_code}", "red")
                safe_cprint(f"  â””â”€ Response body: {e.response.text}", "red")

            if hasattr(e, "__traceback__"):
                import traceback

                safe_cprint(f"\nğŸ“‹ Full traceback:", "red")
                safe_cprint(traceback.format_exc(), "red")

            self.client = None
            raise

    def generate_response(
        self, system_prompt, user_content, temperature=0.7, max_tokens=None
    ):
        """Generate response with no caching"""
        try:
            # Force unique request every time
            timestamp = int(time.time() * 1000)  # Millisecond precision

            response = self.client.chat.completions.create(
                model=self.model_name,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {
                        "role": "user",
                        "content": f"{user_content}_{timestamp}",
                    },  # Make each request unique
                ],
                temperature=temperature,
                max_tokens=max_tokens if max_tokens else self.max_tokens,
                stream=False,  # Disable streaming to prevent caching
            )

            # Extract content and filter out thinking tags
            raw_content = response.choices[0].message.content

            # Remove <think>...</think> tags and their content (Qwen reasoning)
            import re

            # First, try to remove complete <think>...</think> blocks
            filtered_content = re.sub(
                r"<think>.*?</think>", "", raw_content, flags=re.DOTALL
            ).strip()

            # If <think> tag exists but wasn't removed (unclosed tag due to token limit),
            # remove everything from <think> onwards
            if "<think>" in filtered_content:
                filtered_content = filtered_content.split("<think>")[0].strip()

            # If filtering removed everything, return the original (in case it's not a Qwen model)
            final_content = filtered_content if filtered_content else raw_content

            return ModelResponse(
                content=final_content,
                raw_response=response,
                model_name=self.model_name,
                usage=response.usage,
            )

        except Exception as e:
            error_str = str(e)

            # Handle rate limit errors (413)
            if "413" in error_str or "rate_limit_exceeded" in error_str:
                safe_cprint(
                    f"âš ï¸  Groq rate limit exceeded (request too large)", "yellow"
                )
                safe_cprint(f"   Model: {self.model_name}", "yellow")
                if "Requested" in error_str and "Limit" in error_str:
                    # Extract token info from error message
                    import re

                    limit_match = re.search(r"Limit (\d+)", error_str)
                    requested_match = re.search(r"Requested (\d+)", error_str)
                    if limit_match and requested_match:
                        safe_cprint(
                            f"   Limit: {limit_match.group(1)} tokens | Requested: {requested_match.group(1)} tokens",
                            "yellow",
                        )
                safe_cprint(f"   ğŸ’¡ Skipping this model for this request...", "cyan")
                return None

            # Raise 503 errors (service unavailable)
            if "503" in error_str:
                raise e

            # Log other errors
            safe_cprint(f"âŒ Groq error: {error_str}", "red")
            return None

    def is_available(self) -> bool:
        """Check if Groq is available"""
        return self.client is not None

    @property
    def model_type(self) -> str:
        return "groq"
